{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61feddcd",
   "metadata": {},
   "source": [
    "#### Homework2\n",
    "Please explain clearly and include your entire computational work when needed. Should you include any code, please make sure to provide additional comments to explain your solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe85ed4",
   "metadata": {},
   "source": [
    "Q1 (8 points) Answer the following questions clearly. \n",
    "- (4 points) Compare the cost functions in Ridge and Lasso Regression and indicate the regularization parameter. \n",
    "\n",
    "- (4 points) Explain which weights are more penalized in Ridge Regression and why (discuss your answer in the context of constraint satisfaction and take into account the constraint on Ridge Regression coefficients). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ba893",
   "metadata": {},
   "source": [
    "Question 1 Answer: \n",
    "The cost cost functions of the Lasso and the Ridge regression both have a term called the regularization parameter. The regularization parameter essentially adds a penalty to the lost function and it mitigates the loss that is present. For the Lasso regression, it is just a sum of the magnitude of the weights, and for the Ridge regression, it is the Square of the weights.\n",
    "\n",
    "The weights that are more penalized in the Ridge Regression model would be the weights that are larger. Why is this the case? This is the case because of the constraint satisfaction. If the weight is larger, then you essentially have to mitigate the entire loss function to be smaller. (with less error) This is so that constraint can be met. Thus, for weights with a smaller value, they are less penalized. And for weights that are much larger, they will be greatly penalized. All in order to meet this constraint. The rest of the weights of the model have to be adjusted accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ff70a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdac371d",
   "metadata": {},
   "source": [
    "Q2 (12 points) In the context of training a linear regression model using Maximum-Likelihood-Estimation, answer the following questions:\n",
    "- (4 points) Indicate all assumptions discussed in the lecture under the MLE principle about the data, residual error, and the type of the probability density function used in the Likelihood function. \n",
    "- (4 points) Indicate the Likelihood function mathematically with respect to the assumptions made under MLE principle, and describe each term/parameters used in the likelihood function. \n",
    "- (4 points) Explain how the concept of maximizing the likelihood of observing data under model parameters is convertible to minimizing the NLL? Discuss in terms of the mathematical notation and the shape of the function. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe939eb",
   "metadata": {},
   "source": [
    "Question 2 Answer:\n",
    "Part 1:\n",
    "All of the assumptions under the MLE principle about the data, residual error, and the type of probability density function that are used in the Likelihood function are the following. First, we can assume that the data has a normal distribution because we know that the residual error has a normal distribution. This was mentioned in lecture regarding the MLE model. This also directly correlates the type of PDF function used. The notation is a bit odd, but we know that it is the function used for a normal distribution. \n",
    "\n",
    "Part 2:\n",
    "If we were to describe the Likelihood function mathematically with respect to the assumptions made under the MLE principle. Here is the following description. First and foremost, we have the weight parameters, these parameters are what we are trying to predict essentially. The function basically wants to estimate the likelihood of these parameters given the parameters of the prediction model. The coefficents that we want to predict also include the variance! \n",
    "\n",
    "the 'x' and the 'y' variables are the input and output variables. So essentially, when we plug in an X, we will get out a Y. To clarify, the X's are independent, and the Y's are dependent variables, since they rely on the function at hand.\n",
    "\n",
    "For further clarification, across many observations, the Y variable is independent in its own sense. \n",
    "This is because for each observation, there are its own sets of input parameters.\n",
    "\n",
    "Part 3:\n",
    "The concept of maximizing the likelihood function is convertible to minimizing the NLL due to mathematical manipulation.\n",
    "What is the mathematical manipulation? \n",
    "The manipulation occurs like this, we initially take the log of the MLE. The reason that we do this is because it is computationally less expensive on our end. \n",
    "It is easier to handle logs of values as opposed to normal values. After we take the LOG, then we essentially are going to apply a negative sign to it. What this in turn does for us, is that it flips our graph upside down. Now, it is a u-shape. \n",
    "This is convertable because now, when have converted our MLE to a NLL, then we essentially don't to look for a MAX point, we are looking for a minimum of the NLL, this is where we will be able to find the optimal weights, because the Loss will be at its minimum at this particular point.\n",
    "\n",
    "\n",
    "In terms of the mathematical notation, here is our in depth analysis\n",
    "\n",
    "$$\n",
    "\\min_{\\theta} \\sum_y {-\\log(p(y;\\theta))}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\max_{\\theta} \\prod_y p(y;\\theta)\n",
    "$$\n",
    "# analysis:\n",
    "so essentially, we have these two equations, they display how the MLE is equivalent to finding the min of the NLL. This is representation of what the equations look like in essence. IT \n",
    "\n",
    "elaboration:\n",
    "it is computationally more expensive to find the max of the MLE, rather than finding the min of the NLL. This is the main reason that the conversion is necessary in the first place. Both of the methods simply want to find the best parameters for the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d067c09",
   "metadata": {},
   "source": [
    "Q3 (10 points) Use the sklearn Breast_cancer dataset and use min-max scalar to transform the input attributes. Next, develop two classifiers using logistic regression, and perceptron learning. Train on the training data (75% of the entire data) and compare the performance of the models by reporting accuracy \"accuracy = accuracy_score(y_test, y_pred). Which model performs better? Provide your coding for the developed models and document your code. Failing proper documentation leads to losing points. Necessary library functions are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7efb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression performed better on our data with an accuracy of: 0.9790209790209791\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# simply loading in the data\n",
    "# well as loading the data that is going into the target, (the output)\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# scale the data using MinMaxScaler, we are scaling the input data \n",
    "# w/ MinMaxScaler\n",
    "# the MinMax Scaler will make our inputs in the range of \n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# split the data into training (75%) and testing (25%) sets\n",
    "# what we are doing here is splitting the data \n",
    "# we split the data into training data, and we split it into testing data as well\n",
    "# the test size of the data is 25%, and the training size is 75%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, train_size=0.75)\n",
    "# the train_test_split function essentially is what is doing the data splitting for us. \n",
    "# it splits the data into training, and testing data.\n",
    "\n",
    "# The two models that we create will essentially take in \n",
    "# our training data, and develop a model that is fitted to it\n",
    "# this is done below ->\n",
    "\n",
    "# Create and train the logistic regression model\n",
    "logistic_model = LogisticRegression(max_iter=200)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Create and train the perceptron model\n",
    "perceptron_model = Perceptron()\n",
    "perceptron_model.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions on the test set for both models\n",
    "y_pred_logistic_model = logistic_model.predict(X_test)\n",
    "y_pred_perceptron_model = perceptron_model.predict(X_test)\n",
    "\n",
    "# Calculate and compare accuracies of both models\n",
    "accuracy_logistic = accuracy_score(y_test, y_pred_logistic_model)\n",
    "accuracy_perceptron = accuracy_score(y_test, y_pred_perceptron_model)\n",
    "\n",
    "\n",
    "# I added if some conditional statements to see which model performed better on our data\n",
    "# For which model performs better or not, simply depends on the random state of our data.\n",
    "# Essentially, if we set the random_state to a value. Then the data will stay the same.\n",
    "if accuracy_logistic > accuracy_perceptron:\n",
    "    print(\"logistic regression performed better on our data with an accuracy of:\", accuracy_logistic)\n",
    "elif accuracy_logistic < accuracy_perceptron:\n",
    "    print(\"perceptron performed better on our data with an accuracy of:\", accuracy_perceptron)\n",
    "else:\n",
    "    print(\"both logistic regression and perceptron performed equally well on our data with an accuracy of:\", accuracy_logistic)\n",
    "\n",
    "# So, in this code, we basically are able to see which model performs better, the logistic regression, or the perceptron.\n",
    "# whichever model performs better will determine which print statement gets printed out.\n",
    "# EX -> logistic regression performed better, some value\n",
    "# OR -> perceptron performed better etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ac737a",
   "metadata": {},
   "source": [
    "Q4 (6 points) Compare and contrast Newton's method and gradient descent as optimization algorithms for finding the minimum of a function. Provide insights into their convergence properties, computational complexities, and practical considerations. Discuss situations where Newton's method should not be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a83f04b",
   "metadata": {},
   "source": [
    "Question 4 Answer: \n",
    "The gradient descent method is very sensitive to the choice of our learning rate. Hence, this leads us to picking newton’s method. Newton’s method essentially uses the second derivative of the cost function, as well as the first derivative to find and update the parameters of the model.\n",
    "Both of the methods simply want to find the optimal parameters needed for our model. The convergence of each of these algos also varies, for example, the Newton’s method converges much faster than the gradient descent technique. It is heavily computationally complex though, because of the computation in correlation with the Hessian Matrix. Situations that Newton’s method should not be used would be instances where the function has high number of variables. This will lead to a lot of computational complexity. This all ties back to the calculation of things in regards with the Hessian Matrix! Everything will become more computationally expensive. This better suited for a case where there are not many variables involved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b08c1f2",
   "metadata": {},
   "source": [
    "Q5 (6 points) Mathematically explain how a perceptron learning model is trained. Discuss in terms of the gradient of the error function used in Perceptron Learning algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0666e080",
   "metadata": {},
   "source": [
    "\n",
    "Question 5 Answer:\n",
    "The perceptron learning model is trained in the following way, we want to minimize the error function, we do this by taking the gradient of the error function with respect to its weights. \n",
    "For updating the weights, we would then subtract from the old weight, the gradient of the weights, this works very similar to how the weight update rule is in other cases. This process is essentially repeated until the model converges! (This explanation is eerily similar to how the the Professor explained it to be.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56f0128",
   "metadata": {},
   "source": [
    "Q6 (4 points) Compare a Perceptron Learning algorithm with \"Binary Step function\" used as activation function, with a linear regression function in the context of binary classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837dc4fc",
   "metadata": {},
   "source": [
    "Q6 Answer:\n",
    "The perception learning algorithm has a bit more clarity when it comes to classifying. This is because with the activation function, it will map out to the probability of that input. Compared with a threshold value. If it is less than the threshold value, then it will be classified as 0. And if it is greater than the threshold value, then it will be classified as 1. (These values could be varied however).\n",
    "\n",
    "However in the context of binary classification, the perceptron learning algorithm is much better suited for it. For linear regression, the model is very sensitive to threshold values. Thus, the conclusion is that the perceptron learning algorithm is much better suited for Binary Classification. Linear Regression results will always be sensitive to a threshold value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5219f68",
   "metadata": {},
   "source": [
    "Q7 (10 points) Answer the following questions: \n",
    "- (6 points) Discuss the vanishing gradient problem in the context of training deep neural networks and identify activation functions that are particularly susceptible to this phenomenon. \n",
    "  \n",
    "- (4 points) Explain why these activation functions lead to vanishing gradients during backpropagation (hint: discuss in terms of the shape of the activation function). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3795448e",
   "metadata": {},
   "source": [
    "Question 7 Answer: \n",
    "The vanishing gradient descent problem in the context of training a DNN is the following.\n",
    "It essentially is what happens when the gradient becomes so small that the weights get failed to be updated. So essentially, the smaller gradients lead to the model being unable to be trained effectively. This primarily occurs when the activation functions have a distinct range and the gradient remains approaches zero. \n",
    "\n",
    "The following activation functions lead to this particular phenomenon. Sigmoid, tanh, and softplus as well. These activation functions tend to give us the vanishing gradient descent problem because of the fact that they have the vanishing gradient problem when the input value changes severely. For example, for very large inputs, the sigmoid function will get this problem when the inputs are either very positively large or positively negative. The same instance happens for the Tanh function, however for the softplus activation function, the vanishing problem only occurs when the input is extremely negative. This all occurs during the back propagation state, so essentially when the weight values are being updated as the back propagation is occurring. The shape of the activation functions that lead to this issue have varying shapes. The varying shapes also display this phenomena. For each of the functions extreme values, the rate of change of the function is essentially zero (rate of change corresponds with the gradient). At the extreme ends are where we get non-zero gradients that would negatively effect our model. A specific example would be that for the softplus function the values range from 0 to infinity. Thus, for very small input values, the gradient will tend to zero. But not for large positive inputs. This is due to the semantics of how the function is bounded by its ranges. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b950753",
   "metadata": {},
   "source": [
    "Q8 (10 points) List all hyperparameters discussed in the class related to Artificial and Deep Neural Networks and explain the role/impact of each hyperparameter. Which technique(s) can be used to perform hyperparameter tuning? Explain how the technique(s) work.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa57235d",
   "metadata": {},
   "source": [
    "Q9 (20 points) Given a dataset with input attributes x1 and x2, and output variable y, you are training a 3 layer neural network. Assume that activation function used in each layer is sigmoid. Mathematically describe one feed-forward pass followed by one backward-pass in terms of updating the weights of each layer in this neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf26fe8c",
   "metadata": {},
   "source": [
    "Question 9 Answer:\n",
    "Describing one feed forward pass in this network, as well as one backward pass would be working in the following way. So essentially:\n",
    "\n",
    "\n",
    "\n",
    "In the feed forward pass, you have the input to the hidden layer being the following, the x1 and x2 attributes are multiplied by the weights of each neuron and such and there is a bias term influence. The activation function will take this value and place it inside of it, and then this is the output of the hidden layer, to the actual output layer. The same process occurs for the activation layer. And this will produce an output. These are all aspects of the feed forward pass that occur. \n",
    "\n",
    "Backward Propagation:\n",
    "for the backward propagation, the errors are calculated at the output layer. And then the weights and the biases are updated between the hidden layers and the output layers. \n",
    "This is done by taking the derivative of the sigmoid function since this is adjacent to the gradient. The error is then propagated from the output layer to the hidden layers and then the weights and biases are adjusted accordingly. \n",
    "\n",
    "The weight update will occur with the gradients that are computed as well as the learning rate parameter that is tuned to our needs.\n",
    "\n",
    "Overall, this is simply going to refine the predictions of our network over time. Because it will continue to be refined over and over with more optimal weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c7de4e",
   "metadata": {},
   "source": [
    "Q10 (14 points) In this exercise, use the output information generated by this code to perform a comparative study of the performance (i.e., loss , Accuracy) of the neural networks models based on the hyperparameters used. Generate a table to report your analysis.\n",
    "Note: If you want to run this code and have trouble with imported libraries, try 'pip install keras==2.12.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e38d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abdulnawab/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v2.compat.v2.__internal__' has no attribute 'register_load_context_function'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Dropout\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SGD\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/api/_v2/keras/__init__.py:12\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/__init__.py:21\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/models/__init__.py:18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras models API.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Functional\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/engine/functional.py:34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m input_spec\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m node \u001b[38;5;28;01mas\u001b[39;00m node_module\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m training \u001b[38;5;28;01mas\u001b[39;00m training_lib\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m training_utils\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/engine/training.py:40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimizer_v1\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pickle_utils\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_api\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_lib\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization_lib\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/saving/saving_api.py:24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_lib\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save \u001b[38;5;28;01mas\u001b[39;00m legacy_sm_saving_lib\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m io_utils\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/saving/legacy/save.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load \u001b[38;5;28;01mas\u001b[39;00m saved_model_load\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_context\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save \u001b[38;5;28;01mas\u001b[39;00m saved_model_save\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_option_scope\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/saving/legacy/saved_model/load_context.py:68\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns whether under a load context.\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_context\u001b[38;5;241m.\u001b[39min_load_context()\n\u001b[0;32m---> 68\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_load_context_function\u001b[49m(in_load_context)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.compat.v2.__internal__' has no attribute 'register_load_context_function'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Load dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Preprocess data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define create_model function for KerasClassifier\n",
    "def create_model(num_layers=1, num_neurons=64, activation='relu', dropout_rate=0.0, momentum=0.9):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_neurons, input_dim=X_scaled.shape[1], activation=activation))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    for _ in range(num_layers - 1):\n",
    "        model.add(Dense(num_neurons, activation=activation))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    optimizer = SGD(learning_rate=0.01, momentum=momentum)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define parameters grid for grid search\n",
    "param_grid = {\n",
    "    'num_layers': [3],\n",
    "    'num_neurons': [32, 64],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'dropout_rate': [0.2, 0.5],\n",
    "    'momentum': [0.5, 0.9]\n",
    "}\n",
    "\n",
    "# Create KerasClassifier wrapper for scikit-learn\n",
    "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=kfold, scoring='accuracy')\n",
    "grid_result = grid_search.fit(X_scaled, y)\n",
    "\n",
    "# Print results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, std, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, std, param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "871f91ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2533.78s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: keras 2.12.0\n",
      "Uninstalling keras-2.12.0:\n",
      "  Would remove:\n",
      "    /Users/abdulnawab/Library/Python/3.9/lib/python/site-packages/keras-2.12.0.dist-info/*\n",
      "    /Users/abdulnawab/Library/Python/3.9/lib/python/site-packages/keras/*\n",
      "Proceed (Y/n)? "
     ]
    }
   ],
   "source": [
    "pip uninstall keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5b6cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
